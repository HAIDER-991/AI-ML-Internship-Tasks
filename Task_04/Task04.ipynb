{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement\n",
        "Traditional chatbots respond only to the current query and lack memory and access to\n",
        "external knowledge. This limits their usefulness in real-world applications.\n",
        "\n",
        "### Objective\n",
        "The objective of this task is to build a context-aware chatbot using LangChain and\n",
        "Retrieval-Augmented Generation (RAG) that:\n",
        "- Remembers conversation history\n",
        "- Retrieves relevant information from a document corpus\n",
        "- Generates accurate, contextual responses\n"
      ],
      "metadata": {
        "id": "SLoxVN2ap9MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install + Build Vector Store"
      ],
      "metadata": {
        "id": "B_UqOr-5qSbF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4b2xL07PoOo7",
        "outputId": "f10fa091-9cd3-43e5-dd3c-b964d2fe5f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-community langchain-openai faiss-cpu sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# ğŸ” Set OpenAI Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
        "\n",
        "# Create sample corpus\n",
        "with open(\"knowledge.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "Artificial Intelligence is used in healthcare for diagnosis.\n",
        "Machine learning helps predict diseases.\n",
        "AI chatbots assist users with general questions.\n",
        "\"\"\")\n",
        "\n",
        "# Load & split documents\n",
        "loader = TextLoader(\"knowledge.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# Create embeddings + vector store\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n"
      ],
      "metadata": {
        "id": "oZURrT2wpvva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context-Aware RAG Chatbot (Memory + Retrieval)"
      ],
      "metadata": {
        "id": "1LF7hjndqWwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Memory for context\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# RAG chain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Chat function\n",
        "def chat(query):\n",
        "    return qa_chain({\"question\": query})[\"answer\"]\n",
        "\n",
        "# Test\n",
        "chat(\"What is AI used for?\")\n",
        "chat(\"How does it help in healthcare?\")\n"
      ],
      "metadata": {
        "id": "4qROmzTaqFs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Insights\n",
        "- RAG improves accuracy by grounding responses in external knowledge.\n",
        "- Context memory enables multi-turn conversations.\n",
        "- FAISS provides fast and efficient document retrieval.\n",
        "- LangChain simplifies orchestration of LLM, memory, and retriever.\n"
      ],
      "metadata": {
        "id": "hCdlSsOlqEVz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WAAsrAQgqFEq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}